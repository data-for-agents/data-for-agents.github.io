<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">

  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on *Qwen 3 1.7B* that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy *Qwen 3 235B*, a 235 times larger *Llama 4 Maverick*, and reaching 94.7% of the performance of *Gemini 2.5 Flash*. We are releasing code, models and data at: [data-for-agents.github.io](https://data-for-agents.github.io).">

  <meta property="og:title" content="InSTA: Towards Internet-Scale Training For Agents"/>
  <meta property="og:description" content="The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on *Qwen 3 1.7B* that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy *Qwen 3 235B*, a 235 times larger *Llama 4 Maverick*, and reaching 94.7% of the performance of *Gemini 2.5 Flash*. We are releasing code, models and data at: [data-for-agents.github.io](https://data-for-agents.github.io)."/>
  <meta property="og:url" content="data-for-agents.github.io"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/data-for-agents-teaser.gif" />
  <meta property="og:image:width" content="800"/>
  <meta property="og:image:height" content="600"/>

  <meta name="twitter:title" content="InSTA: Towards Internet-Scale Training For Agents">
  <meta name="twitter:description" content="The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on *Qwen 3 1.7B* that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy *Qwen 3 235B*, a 235 times larger *Llama 4 Maverick*, and reaching 94.7% of the performance of *Gemini 2.5 Flash*. We are releasing code, models and data at: [data-for-agents.github.io](https://data-for-agents.github.io).">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/data-for-agents-teaser.gif">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Deep Learning, Generative AI, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Towards Internet-Scale Training For Agents</title>
  <link type="image/x-icon" href="https://www.ml.cmu.edu/images/icons/favicon.ico" rel="icon"/>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
</head>
<body class="currently-anonymous">


  <section class="hero">
    <div class="hero-body title-section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title">InSTA: Towards Internet-Scale Training For Agents</h1>

            <div class="anonymous is-size-5 publication-authors">
              <span class="author-block"> Anonymous Authors </span>
            </div>

            <div class="non-anonymous is-size-5 publication-authors">
              <span class="author-block"><a href="https://btrabuc.co" target="_blank">Brandon Trabucco</a> <sup>1</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=clTKG0QAAAAJ&hl=en" target="_blank">Gunnar Sigurdsson</a> <sup>2</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=2CkqEGcAAAAJ&hl=en" target="_blank">Robinson Piramuthu</a> <sup>2</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://www.cs.cmu.edu/~rsalakhu/" target="_blank">Ruslan Salakhutdinov</a> <sup>1</sup></span>
            </div>

            <div class="non-anonymous is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Carnegie Mellon University, Machine Learning Department &nbsp;&nbsp; <sup>2</sup> Amazon
            </div>

            <div class="anonymous column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="#" id="toggle-anon" class="external-link button is-normal is-rounded">
                    <span>Click To De-Anonymize</span>
                  </a>
                </span>
  
              </div>
            </div>

            <div class="non-anonymous column has-text-centered">
              <div class="publication-links">

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.06776" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>

                &nbsp;&nbsp;

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/data-for-agents/insta-150k-v3" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span>Data</span>
                  </a>
                </span>

                &nbsp;&nbsp;

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/data-for-agents/insta" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                &nbsp;&nbsp;

                <!-- BibTex link -->
                <span class="link-block">
                  <a href="static/bibtex.txt" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-google-scholar"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
  
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>

    <img src="static/images/data-for-agents-teaser.gif" alt="InSTA: Towards Internet-Scale Training For Agents Teaser GIF" class="teaser-gif image">

  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h1 class="title is-3" style="text-align: center;">Abstract</h1>
      <p class="abstract">
        The predominant approach for training web navigation agents is to gather human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data is an inefficient resource. We develop a pipeline to facilitate internet-scale training for agents without laborious human annotations. In the first stage, an LLM annotates 150k sites with agentic tasks. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM filters trajectories by judging their success. Language models are powerful data curation tools, identifying harmful content with an accuracy of 97%, judging successful trajectories with an accuracy of 82.6%, and producing effective data. We train agents based on <i>Qwen 3 1.7B</i> that are competitive with frontier LLMs as web agents, while being smaller and faster. Our top agent reaches a success rate of 56.9%, outperforming the data collection policy <i>Qwen 3 235B</i>, a 235 times larger <i>Llama 4 Maverick</i>, and reaching 94.7% of the performance of <i>Gemini 2.5 Flash</i>. We are releasing code, models and data at: <a href="https://data-for-agents.github.io" style="color: darkblue;">data-for-agents.github.io</a>.
      </p>
    </div>
  </section>

  <!-- Internet-Scale Task Generation -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/pipeline_overview.png" alt="Towards Internet-Scale Training For Agents" class="image">
        <br>
        <p>
          <strong>Overview of the InSTA pipeline.</strong> Our work unlocks a dynamic internet-scale environment that allows training small models that match top industry LLMs as agents, on a fraction of the budget. Starting from the top 1M sites on the internet, we efficiently annotate 150k sites with challenging agentic tasks, and release the entire pipeline, including code, models and an official huggingface dataset.
        </p>
    </div>
    </div>
  </section>

  <!-- Section 4 - Internet-Scale Task Generation -->
  <section class="section hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/wordcloud.png" alt="Section 4 - Internet-Scale Task Generation" class="image">
        <br>
        <p><strong>Most frequent words in our tasks.</strong> This wordcloud shows the top 500 most frequent words in tasks from the training set of our official huggingface dataset. The size of each word corresponds to its frequency in the dataset. Our tasks span diverse categories and lexicon.</p>
      </div>
    </div>
  </section>

  <!-- Section 5 - Internet-Scale Environment -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/environment_overview.png" alt="Section 5 - Internet-Scale Environment" class="image">
        <br>
        <p><strong>Unlocking a dynamic internet-scale environment.</strong> Building on the large and diverse set of tasks, we employ pretrained language models to attempt and evaluate tasks. We run language model agents to perform tasks using the Playwright API. We then employ language model judges to evaluate trajectories.</p>
      </div>
    </div>
  </section>

  <!-- Section 6 - Training Agents -->
  <section class="section hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/training-results.png" alt="Section 6 - Training Agents" class="image">
        <br>
        <p><strong>InSTA unlocks great potential in small models.</strong> We train agents based on <i>Qwen 3 1.7B</i> using trajectories produced by a <i>Qwen 3 235B</i> data collection policy, and optionally filtered by a <i>Qwen 3 235B</i> judge (see Judge Filtered vs. Uniformly Sampled). We report success rates on a test set of 3,000 held-out websites and tasks. Before training, <i>Qwen 3 1.7B</i> has a zero-shot success rate of 11.5% according to a <i>Qwen 3 235B</i> judge, and we improve this by +45.3% absolute percentage points. Our top checkpoint outperforms the <i>Qwen 3 235B</i> data collection policy, and <i>Llama 4 Maverick</i>, a frontier LLM with 400B parameters, for which our model is 235 times smaller. Notably, filtering with a <i>Qwen 3 235B</i> judge leads to agents that improve according to independent judges, including <i>Gemini 2.5 Flash</i>, <i>Llama 4 Maverick</i>, and <i>Gpt 4.1 Nano</i>, suggesting it generalizes well.</p>
      </div>
    </div>
  </section>

  <!-- BibTex -->
  <section class="non-anonymous section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citing our work</h2>
      <pre><code>@misc{Trabucco2025InSTA,
    title={InSTA: Towards Internet-Scale Training For Agents},
    author={Brandon Trabucco and Gunnar Sigurdsson and Robinson Piramuthu and Ruslan Salakhutdinov},
    year={2025},
    eprint={2502.06776},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container is-max-desktop content">
      <p>This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a></p>
    </div>
  </footer>

  <script>

    let is_currently_anon = true;

    let body = document.getElementsByTagName("body")[0];

    document.getElementById("toggle-anon").addEventListener("click", function() {

      if (is_currently_anon) {

        body.classList.remove("currently-anonymous");
        is_currently_anon = false;

      } else {

        body.classList.add("currently-anonymous");
        is_currently_anon = true;
      }

    });

  </script>

</body>
</html>
