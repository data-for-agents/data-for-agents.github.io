<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">

  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.">

  <meta property="og:title" content="Towards Internet-Scale Training For Agents"/>
  <meta property="og:description" content="The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web."/>
  <meta property="og:url" content="data-for-agents.github.io"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/data-for-agents-teaser.gif" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Towards Internet-Scale Training For Agents">
  <meta name="twitter:description" content="The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/data-for-agents-teaser.gif">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Deep Learning, Generative AI, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Towards Internet-Scale Training For Agents</title>
  <link type="image/x-icon" href="https://www.ml.cmu.edu/images/icons/favicon.ico" rel="icon"/>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
</head>
<body class="currently-anonymous">


  <section class="hero">
    <div class="hero-body title-section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-1 publication-title">Towards Internet-Scale Training For Agents</h1>

            <div class="anonymous is-size-5 publication-authors">
              <span class="author-block"> Anonymous Authors </span>
            </div>

            <div class="non-anonymous is-size-5 publication-authors">
              <span class="author-block"><a href="https://btrabuc.co" target="_blank">Brandon Trabucco</a> <sup>1</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=clTKG0QAAAAJ&hl=en" target="_blank">Gunnar Sigurdsson</a> <sup>2</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=2CkqEGcAAAAJ&hl=en" target="_blank">Robinson Piramuthu</a> <sup>2</sup> &nbsp;&nbsp; </span>
              <span class="author-block"><a href="https://www.cs.cmu.edu/~rsalakhu/" target="_blank">Ruslan Salakhutdinov</a> <sup>1</sup></span>
            </div>

            <div class="non-anonymous is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Carnegie Mellon University, Machine Learning Department &nbsp;&nbsp; <sup>2</sup> Amazon
            </div>

            <div class="anonymous column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="#" id="toggle-anon" class="external-link button is-normal is-rounded">
                    <span>Click To De-Anonymize</span>
                  </a>
                </span>
  
              </div>
            </div>

            <div class="non-anonymous column has-text-centered">
              <div class="publication-links">

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.06776" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>

                &nbsp;&nbsp;

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/data-for-agents/data-for-agents.github.io" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                &nbsp;&nbsp;

                <!-- BibTex link -->
                <span class="link-block">
                  <a href="static/bibtex.txt" target="_blank"
                    class="external-link button is-normal is-rounded">
                    <span class="icon">
                      <i class="ai ai-google-scholar"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
  
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>

    <img src="static/images/data-for-agents-teaser.gif" alt="Towards Internet-Scale Training For Agents Teaser GIF" class="teaser-gif image">

  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h1 class="title is-3" style="text-align: center;">Abstract</h1>
      <p class="abstract">
        The predominant approach for training web navigation agents gathers human demonstrations for a set of popular websites and hand-written tasks, but it is becoming clear that human data are an inefficient resource. We develop a pipeline to facilitate Internet-scale training for agents without laborious human annotations. In the first stage, an LLM generates tasks for 150k diverse websites. In the next stage, LLM agents complete tasks and produce trajectories. In the final stage, an LLM reviews the trajectories and judges their success. Language models are competitive with human annotators, detecting and filtering out harmful content with an accuracy of 97%, generating feasible tasks with an 89% rate, and judging successful trajectories with an 82.6% accuracy. Scaling the pipeline, agents based on <i>Llama 3.1 70B</i> solve 16.7% of tasks for 150k sites. Training on the data generated by our pipeline is competitive with training on human demonstrations. In data-limited settings derived from Mind2Web and WebLINX, we improve <i>Step Accuracy</i> by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks, agents fail to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.
      </p>
    </div>
  </section>

  <!-- Internet-Scale Task Generation -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/pipeline_overview.png" alt="Towards Internet-Scale Training For Agents" class="image">
        <br>
        <p>
          <strong>Overview of the proposed agent pipeline.</strong> &nbsp;&nbsp; We develop a pipeline for training web navigation agents at internet scale using tasks proposed, attempted, and evaluated by pretrained large language models. We generate 150k diverse tasks across 1M internet sites. Code for our data generation pipeline, and traces for agent rollouts will be available on our website: <a href="https://data-for-agents.github.io" style="color: darkblue;">data-for-agents.github.io</a>.
        </p>
    </div>
    </div>
  </section>

  <!-- Section 4 - Internet-Scale Task Generation -->
  <section class="section hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/task_generation_overview.png" alt="Section 4 - Internet-Scale Task Generation" class="image">
        <br>
        <p><strong>Task proposal and filtering for 150k live websites.</strong> &nbsp;&nbsp; Starting from 1,000,000 websites, we employ a pretrained language model that marks sites as safe/unsafe for annotation, and assigns a realistic task that a hypothetical user might want to accomplish on each site. The task proposer rejects 85% of websites from the pipeline, resulting in 150k safe websites annotated with realistic tasks.</p>
        <p class="non-anonymous">Code for task generation is available at: <a href="https://github.com/data-for-agents/task-generation" style="color: darkblue">github.com/data-for-agents/task-generation</a></p>
      </div>
    </div>
  </section>

  <!-- Section 5 - Internet-Scale Agents -->
  <section class="section hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/environment_overview.png" alt="Section 5 - Internet-Scale Agents" class="image">
        <br>
        <p><strong>Automatic evaluation for agents with language model judges.</strong> &nbsp;&nbsp; Building on the large and diverse set of tasks generated by the pipeline, we employ pretrained language models to attempt and evaluate web navigation tasks. We dispatch language model agents to perform tasks by making calls to the Playwright API. We then employ language model judges to evaluate rollouts from agents.</p>
        <p class="non-anonymous">Code for the environment is available at: <a href="https://github.com/data-for-agents/environment" style="color: darkblue">github.com/data-for-agents/environment</a></p>
      </div>
    </div>
  </section>

  <!-- Section 6 - Training Agents -->
  <section class="section hero is-small is-light">
    <div class="hero-body">
      <div class="container is-max-desktop content">
        <img src="static/images/training-results.png" alt="Section 6 - Training Agents" class="image">
        <br>
        <p><strong>Training agents with internet-scale data.</strong> &nbsp;&nbsp; In data-limited settings derived from Mind2Web and WebLINX (<i>left plot</i>), we improve Step Accuracy by up to +89.5% and +122.1% respectively for agents trained on mixtures of data from our pipeline, and human data. When training agents with all available human data from these benchmarks (<i>right plot</i>), agents trained on existing human data struggle to generalize to diverse real sites, and adding our data improves their generalization by +149.0% for WebLINX and +156.3% for Mind2Web.</p>
        <p class="non-anonymous">Code for training is available at: <a href="https://github.com/data-for-agents/training" style="color: darkblue">github.com/data-for-agents/training</a></p>
      </div>
    </div>
  </section>

  <!-- BibTex -->
  <section class="non-anonymous section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citing our work</h2>
      <pre><code>@misc{Trabucco2025InSTA,
    title={InSTA: Towards Internet-Scale Training For Agents},
    author={Brandon Trabucco and Gunnar Sigurdsson and Robinson Piramuthu and Ruslan Salakhutdinov},
    year={2025},
    eprint={2502.06776},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container is-max-desktop content">
      <p>This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a></p>
    </div>
  </footer>

  <script>

    let is_currently_anon = true;

    let body = document.getElementsByTagName("body")[0];

    document.getElementById("toggle-anon").addEventListener("click", function() {

      if (is_currently_anon) {

        body.classList.remove("currently-anonymous");
        is_currently_anon = false;

      } else {

        body.classList.add("currently-anonymous");
        is_currently_anon = true;
      }

    });

  </script>

</body>
</html>
